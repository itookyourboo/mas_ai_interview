import asyncio
import json
from typing import List, TypedDict
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph

from src import settings


llm = ChatOpenAI(
    api_key=settings.MODEL_API_KEY,
    model=settings.MODEL_NAME,
    base_url=settings.MODEL_BASE_URL,
    temperature=0.3,
)

# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¸Ð½Ñ‚ÐµÑ€Ð²ÑŒÑŽ
INTERVIEW_PARAMS = {
    "position": "Backend-Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº",
    "tech_stack": "Python, FastAPI, PostgreSQL, Redis",
    "level": "Middle",
    "topics": ["API", "ÐšÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ", "Ð‘Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…", "ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾ÑÑ‚ÑŒ"],
    "time_limit": 60,
    "num_questions": 5,
}


# ==============================
# Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð³Ñ€Ð°Ñ„Ð°
# ==============================

class QuestionState(TypedDict):
    index: int
    plan: str
    question: str
    validated: str  # "ÐžÐ”ÐžÐ‘Ð Ð•ÐÐž" Ð¸Ð»Ð¸ "ÐžÐ¢ÐšÐ›ÐžÐÐÐ: ..."
    formatted: dict | None
    attempts: int


class OverallState(TypedDict):
    questions_to_generate: int
    completed_questions: List[dict]
    current_index: int


# ==============================
# ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚Ñ‹ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼
# ==============================

# 1. ÐŸÐ»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
design_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Ð’Ñ‹ â€” Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚Ð¾Ñ€ ÑÐ¾Ð±ÐµÑÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¹. Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ Ð¿Ð»Ð°Ð½ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°."),
        (
            "human",
            "ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹:\nÐŸÐ¾Ð·Ð¸Ñ†Ð¸Ñ: {position}\nÐ¢ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¸: {tech_stack}\nÐ£Ñ€Ð¾Ð²ÐµÐ½ÑŒ: {level}\n"
            "Ð¢ÐµÐ¼Ñ‹: {topics}\nÐ’Ñ€ÐµÐ¼Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð²ÑŒÑŽ: {time_limit} Ð¼Ð¸Ð½.\n"
            "Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ Ð¿Ð»Ð°Ð½ Ð´Ð»Ñ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° â„–{index}. Ð£ÐºÐ°Ð¶Ð¸Ñ‚Ðµ Ñ‚Ð¸Ð¿, ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ, ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¸ Ð¸ Ð²Ñ€ÐµÐ¼Ñ Ð½Ð° Ð¾Ñ‚Ð²ÐµÑ‚.",
        ),
    ],
)

# 2. Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ
generate_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Ð’Ñ‹ â€” Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð². Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ Ñ‡Ñ‘Ñ‚ÐºÐ¸Ð¹ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼."),
        ("human", "ÐŸÐ»Ð°Ð½ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°:\n{plan}\nÐ¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ."),
    ],
)

# 3. Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ
validate_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Ð’Ñ‹ â€” Ð²Ð°Ð»Ð¸Ð´Ð°Ñ‚Ð¾Ñ€. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ."),
        (
            "human",
            "ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ:\n{question}\n"
            "ÐžÑ†ÐµÐ½Ð¸Ñ‚Ðµ: ÑÑÐ½Ð¾ÑÑ‚ÑŒ, ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ ÑƒÑ€Ð¾Ð²Ð½ÑŽ Middle, Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ ÑÑ‚ÐµÐºÑƒ Python/FastAPI, "
            "Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð²Ð·ÑÑ‚Ð¾ÑÑ‚Ð¸ Ð¸ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ. "
            "Ð•ÑÐ»Ð¸ Ð²ÑÑ‘ Ð² Ð¿Ð¾Ñ€ÑÐ´ÐºÐµ â€” Ð½Ð°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ 'ÐžÐ”ÐžÐ‘Ð Ð•ÐÐž'. Ð˜Ð½Ð°Ñ‡Ðµ â€” 'ÐžÐ¢ÐšÐ›ÐžÐÐÐ: [Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð°]'.",
        ),
    ],
)

# 4. Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
format_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Ð’Ñ‹ â€” Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸Ðº. Ð’Ñ‹Ð²ÐµÐ´Ð¸Ñ‚Ðµ ÑÑ‚Ñ€Ð¾Ð³Ð¾ JSON."),
        (
            "human",
            "ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐ¹Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð² JSON ÑÐ¾ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Ð¿Ð¾Ð»ÑÐ¼Ð¸:\n"
            "- question: Ñ‚ÐµÐºÑÑ‚ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼\n"
            "- type: 'ÐºÐ¾Ð´', 'ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð´Ð¸Ð·Ð°Ð¹Ð½', 'Ñ‚ÐµÐ¾Ñ€Ð¸Ñ', 'Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ°'\n"
            "- level: 'Middle'\n"
            "- tags: ÑÐ¿Ð¸ÑÐ¾Ðº Ñ‚ÐµÐ¼ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼\n"
            "- expected_time_min: Ñ†ÐµÐ»Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾\n"
            "- follow_ups: ÑÐ¿Ð¸ÑÐ¾Ðº Ð¸Ð· 1â€“3 follow-up Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼\n\n"
            "Ð’Ð¾Ð¿Ñ€Ð¾Ñ:\n{validated_question}\n\n"
            "Ð’Ñ‹Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð¢ÐžÐ›Ð¬ÐšÐž JSON, Ð±ÐµÐ· Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸Ð¹.",
        ),
    ],
)


# ==============================
# Ð£Ð·Ð»Ñ‹ Ð³Ñ€Ð°Ñ„Ð°
# ==============================

async def design_node(state: QuestionState) -> dict:
    chain = design_prompt | llm
    response = await chain.ainvoke(
        {
            "position": INTERVIEW_PARAMS["position"],
            "tech_stack": INTERVIEW_PARAMS["tech_stack"],
            "level": INTERVIEW_PARAMS["level"],
            "topics": ", ".join(INTERVIEW_PARAMS["topics"]),
            "time_limit": INTERVIEW_PARAMS["time_limit"],
            "index": state["index"] + 1,
        },
    )
    return {"plan": response.content}


async def generate_node(state: QuestionState) -> dict:
    chain = generate_prompt | llm
    response = await chain.ainvoke({"plan": state["plan"]})
    return {"question": response.content}


async def validate_node(state: QuestionState) -> dict:
    chain = validate_prompt | llm
    response = await chain.ainvoke({"question": state["question"]})
    validated = response.content.strip()
    return {"validated": validated}


async def format_node(state: QuestionState) -> dict:
    if not state["validated"].startswith("ÐžÐ”ÐžÐ‘Ð Ð•ÐÐž"):
        return {"formatted": None}
    chain = format_prompt | llm
    try:
        response = await chain.ainvoke({"validated_question": state["question"]})
        # ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ð°Ñ€ÑÐ¸Ð¼ JSON
        formatted = json.loads(response.content.strip())
        return {"formatted": formatted}
    except Exception as e:
        return {"formatted": None}


# ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° (Ð¿Ð¾Ð´Ð³Ñ€Ð°Ñ„)
def create_question_graph() -> StateGraph:
    graph = StateGraph(QuestionState)
    graph.add_node("design", design_node)
    graph.add_node("generate", generate_node)
    graph.add_node("validate", validate_node)
    graph.add_node("format", format_node)

    graph.set_entry_point("design")
    graph.add_edge("design", "generate")
    graph.add_edge("generate", "validate")
    graph.add_edge("validate", "format")
    graph.add_edge("format", END)

    return graph.compile()


# ==============================
# ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð³Ñ€Ð°Ñ„ (ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ N Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸)
# ==============================

async def run_interview_generator() -> List[dict]:
    question_graph = create_question_graph()
    completed = []
    index = 0

    while len(completed) < INTERVIEW_PARAMS["num_questions"] and index < INTERVIEW_PARAMS["num_questions"] * 3:
        print(f"Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° #{len(completed) + 1} (Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ° {index + 1})...")

        initial_state: QuestionState = {
            "index": len(completed),
            "plan": "",
            "question": "",
            "validated": "",
            "formatted": None,
            "attempts": 0,
        }

        result = await question_graph.ainvoke(initial_state)

        if result["formatted"]:
            completed.append(result["formatted"])
            print("âœ… Ð’Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ñ€Ð¸Ð½ÑÑ‚.")
        else:
            print("âŒ Ð’Ð¾Ð¿Ñ€Ð¾Ñ Ð¾Ñ‚ÐºÐ»Ð¾Ð½Ñ‘Ð½ Ð¸Ð»Ð¸ Ð½Ðµ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐµÐ½. ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€...")

        index += 1

    return completed


# ==============================
# Ð—Ð°Ð¿ÑƒÑÐº
# ==============================

async def main():
    print("ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð½Ð° LangGraph...")
    questions = await run_interview_generator()

    output = {"questions": questions}
    with open("interview_questions_langgraph.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ {len(questions)} Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð².")
    print("ðŸ’¾ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: interview_questions_langgraph.json")


if __name__ == "__main__":
    asyncio.run(main())
